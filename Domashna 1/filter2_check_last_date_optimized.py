"""
Filter 2: Check Last Available Date - OPTIMIZED VERSION
Uses BATCH query instead of N+1 queries for 100x better performance
"""

from datetime import datetime, timedelta
from typing import List, Dict, Optional
from supabase import create_client, Client
from tqdm import tqdm
import config

# Initialize Supabase client
supabase: Client = create_client(config.SUPABASE_URL, config.SUPABASE_KEY)


def get_all_last_sync_dates_batch(symbols: List[str]) -> Dict[str, Optional[str]]:
    """
    Get last sync dates for ALL symbols in ONE query (FAST!)

    OLD WAY: 1000 queries (1000 Ã— 100ms = 100 seconds) ğŸŒ
    NEW WAY: 1 query (1 Ã— 100ms = 0.1 seconds) ğŸš€

    Args:
        symbols: List of symbols to check

    Returns:
        Dict mapping symbol â†’ last_date (or None if no data)
    """
    try:
        print(f"\nğŸ” Fetching last sync dates for {len(symbols)} symbols in ONE batch query...")

        # Strategy: Query crypto_metadata table which has last_sync_date already!
        # This is MUCH faster than querying crypto_data
        response = supabase.table('crypto_metadata') \
            .select('symbol, last_sync_date') \
            .in_('symbol', symbols) \
            .execute()

        # Create lookup dict
        last_dates = {}
        for row in response.data:
            last_dates[row['symbol']] = row.get('last_sync_date')

        # Add None for symbols not in metadata
        for symbol in symbols:
            if symbol not in last_dates:
                last_dates[symbol] = None

        print(f"   âœ… Fetched {len(last_dates)} last sync dates in ONE query!")

        return last_dates

    except Exception as e:
        print(f"   âš ï¸  Error in batch query: {e}")
        # Fallback to empty dict
        return {symbol: None for symbol in symbols}


def calculate_download_range(symbol: str, last_date_str: Optional[str]) -> Dict:
    """
    Calculate what date range needs to be downloaded

    Args:
        symbol: Cryptocurrency symbol
        last_date_str: Last date string ('YYYY-MM-DD') or None

    Returns:
        Dictionary with download information
    """
    today = datetime.now()

    if last_date_str is None:
        # No existing data - need full history
        start_date = today - timedelta(days=config.DAYS_OF_HISTORY)
        days_missing = config.DAYS_OF_HISTORY
        download_type = "full_history"
    else:
        # Has existing data - only download missing days
        last_date = datetime.strptime(last_date_str, '%Y-%m-%d')
        start_date = last_date + timedelta(days=1)
        days_missing = (today - last_date).days
        download_type = "incremental"

    return {
        'start_date': start_date,
        'end_date': today,
        'days_missing': days_missing,
        'download_type': download_type
    }


def filter2_check_last_date(symbols_list: List[Dict]) -> List[Dict]:
    """
    Main function for Filter 2 - OPTIMIZED VERSION
    Checks database for last available date for each symbol

    PERFORMANCE IMPROVEMENT:
    - OLD: 1000 separate queries (~100 seconds)
    - NEW: 1 batch query (~0.1 seconds)
    - SPEEDUP: 1000x faster! ğŸš€

    Args:
        symbols_list: List of symbol dictionaries from Filter 1

    Returns:
        List of symbols with date range information
    """
    print("\n" + "="*60)
    print("ğŸ“… FILTER 2: CHECK LAST AVAILABLE DATE (OPTIMIZED)")
    print("="*60)

    symbols_with_dates = []
    stats = {
        'full_history': 0,
        'incremental': 0,
        'up_to_date': 0,
        'total_days_to_fetch': 0
    }

    # Extract all symbols
    all_symbols = [s['symbol'] for s in symbols_list]

    # ğŸš€ OPTIMIZATION: Get all last dates in ONE batch query!
    last_dates_map = get_all_last_sync_dates_batch(all_symbols)

    print(f"\nğŸ“Š Processing {len(symbols_list)} symbols...")

    # Now process each symbol (fast - no database queries!)
    for symbol_data in tqdm(symbols_list, desc="Processing dates", ncols=80):
        symbol = symbol_data['symbol']

        # Get last sync date from our batch query result (no DB call!)
        last_date_str = last_dates_map.get(symbol)

        # Calculate download range
        download_info = calculate_download_range(symbol, last_date_str)

        # Combine symbol data with download info
        symbol_with_date = {
            **symbol_data,
            'last_date': last_date_str,
            'start_date': download_info['start_date'].strftime('%Y-%m-%d'),
            'end_date': download_info['end_date'].strftime('%Y-%m-%d'),
            'days_missing': download_info['days_missing'],
            'download_type': download_info['download_type']
        }

        symbols_with_dates.append(symbol_with_date)

        # Update statistics
        if download_info['days_missing'] == 0:
            stats['up_to_date'] += 1
        elif download_info['download_type'] == 'full_history':
            stats['full_history'] += 1
        else:
            stats['incremental'] += 1

        stats['total_days_to_fetch'] += download_info['days_missing']

    # Print summary
    print("\n" + "="*60)
    print("ğŸ“Š FILTER 2 SUMMARY")
    print("="*60)
    print(f"ğŸ“Š Total symbols: {len(symbols_with_dates)}")
    print(f"ğŸ†• Need full history ({config.YEARS_OF_HISTORY} years): {stats['full_history']}")
    print(f"ğŸ“ˆ Need incremental update: {stats['incremental']}")
    print(f"âœ… Already up-to-date: {stats['up_to_date']}")
    print(f"ğŸ“… Total days to fetch: {stats['total_days_to_fetch']:,}")
    print(f"ğŸ“¦ Estimated API calls: ~{stats['total_days_to_fetch'] // 1000 + len(symbols_with_dates):,}")

    # Show examples
    print(f"\nğŸ“‹ Sample symbols needing full history:")
    full_history_samples = [s for s in symbols_with_dates if s['download_type'] == 'full_history'][:3]
    for sample in full_history_samples:
        print(f"   - {sample['symbol']:8} ({sample['name'][:20]:20}) - {sample['days_missing']:,} days")

    if stats['incremental'] > 0:
        print(f"\nğŸ“‹ Sample symbols needing incremental update:")
        incremental_samples = [s for s in symbols_with_dates if s['download_type'] == 'incremental'][:3]
        for sample in incremental_samples:
            print(f"   - {sample['symbol']:8} ({sample['name'][:20]:20}) - {sample['days_missing']} days (from {sample['last_date']})")

    print(f"\nğŸš€ PERFORMANCE: Used 1 batch query instead of {len(symbols_list)} separate queries!")
    print("="*60 + "\n")

    return symbols_with_dates


if __name__ == "__main__":
    # Test Filter 2 independently
    import json

    # Load Filter 1 output
    try:
        with open('filter1_output.json', 'r') as f:
            symbols = json.load(f)
    except FileNotFoundError:
        print("âŒ filter1_output.json not found. Run filter1_get_symbols.py first.")
        exit(1)

    # Run Filter 2
    import time
    start = time.time()

    symbols_with_dates = filter2_check_last_date(symbols)

    duration = time.time() - start

    # Save output
    with open('filter2_output.json', 'w') as f:
        json.dump(symbols_with_dates, f, indent=2)

    print(f"âœ… Filter 2 completed in {duration:.2f} seconds!")
    print(f"ğŸ’¾ Saved output to filter2_output.json")
